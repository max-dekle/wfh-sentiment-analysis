# Tech Report:


When first scraping, we obtained 500 data points representing The Guardian articles, 645 data points representing Reddit posts and 900 data points representing the New York Times articles, for a total of 2045 articles. After cleaning and deduplication, we obtained 1150 records, 548 from Reddit, 307 from the NYT, and and 295 from the Guardian. Our data is cleaned and free of any duplicates. We believe this data is sufficient to perform our data as it gives us plenty of input to feed an AI/ML algorithm. 
  
Our identifying attributes include the ID, URL, DATE, PLAIN_TEXT and SOURCE. The ID is a unique ID that distinguishes the articles from each other, retrieved from the source (the Guardian, Reddit and the New York Times). 
  
The Guardian data is from the Guardian API, using “(work OR working) AND (home OR remote OR remotely) AND NOT cup” as a query. The Guardian is a reliable British newspaper and the Guardian API has accurate representations of their articles. The Reddit data from the Reddit API, specifically pertaining to posts from the “experienceddevs,” “programming”, and “cscareerquestions” subreddits that had one or more of the keywords “remote work,” “work from home” or “WFH.” In order to ensure that the posts included in the dataset were reputable, we only included posts that received more than 200 upvotes. The New York Times data was collected using their API, focusing on the query “remote work.” The API has an accurate, reliable representation of the articles relevant to the topic. For our three different sources, we aimed to make our data collection and cleaning processes as homogeneous as possible and tried to account for key differences in the sources. A specific challenge that we faced was ensuring that most of the articles found during scraping were actually relevant to remote work, especially from NYT/ The Guardian; to do this, we filtered such that only posts after Jan 1, 2020 were included, and filtered out certain confounding keywords during scraping. Despite this, a few irrelevant articles did leak through; however, we believe that this is a very small portion of the overall dataset and can be accounted for during analysis. The sample has 100 rows and was generated by randomly selecting articles from the three sources.

Another significant consideration for our data is the presence of the sentiment_ratings table. For demonstration purposes, we have included the voting scores of articles from reddit in the table/sample; however, the main purpose of the table is to store the sentiment ratings that we plan on generating during analysis, and as such is not fully populated at this time.
  
We implemented the same cleaning strategy for all data regardless of the source in order to ensure consistency among the data. We removed any non-word syntax and special characters such as “\n,” “<.*?>” among others. We also removed any unnecessary spaces and made sure all text was in unicode. By the end of our cleaning process, all the PLAIN_TEXT only included words and necessary punctuation.

Before we checked the cleanliness of our data, we checked the data structure and verified that all data was well-formed and had values for the 5 attributes. We then checked for missing values by using “df.isnull().sum().” We verified that our data had no missing data. We checked for duplicates using “df.duplicated().sum().” We also checked that PLAIN_TEXT was clean and that all special characters were removed. Finally, once all data was collated and cleaned, we ensured that all PLAIN_TEXT was unique. We threw away any data point that had a creation date earlier than Jan 1, 2020, as we are only interested in discussion of remote work during the COVID-19 pandemic. We may need to throw away a small amount of the data later if we find that there are too many irrelevant articles present in the final analysis. However, this would require manual review of each data point, which we did not have time to perform before this deliverable.

We have been pleasantly surprised with the sheer amount of articles and posts regarding remote work and are confident that we have sufficient data to feed our AI/ML model in order to characterize attitudes toward remote work in different online communities. The APIs made the retrieval process much easier. Moreover, using the same cleaning method for all data simplified the cleaning process. One challenge was combining all the data together and ensuring that there were no ID duplicates and the IDs were retrieved from the sources and not randomly generated by us. The other primary challenge was ensuring the articles were topically relevant and from the correct time period for our analysis. Our next step is to feed all our input into an AI/ML model in order to begin analyzing the data.
